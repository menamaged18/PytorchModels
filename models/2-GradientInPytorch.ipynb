{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4eab3a-3cea-447b-ad80-7231e56e8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14661140-9171-40ae-a22d-a2f5b293d91a",
   "metadata": {},
   "source": [
    "### requires_grad will tell the pytorch that we need to calculate the gradient  for this tensor we use itlater  in an optimization steps problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ee692d0-dfa2-48f4-b4c7-8688148ea598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1194, 0.7722, 0.6958], requires_grad=True)\n",
      "tensor([2.1194, 2.7722, 2.6958], grad_fn=<AddBackward0>)\n",
      "tensor([ 8.9839, 15.3703, 14.5347], grad_fn=<MulBackward0>)\n",
      "tensor(12.9629, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad = True)\n",
    "print(x)\n",
    "y = 2 + x\n",
    "print(y)\n",
    "z = y * y * 2\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ed4b3-6410-455f-baa0-deddd986b282",
   "metadata": {},
   "source": [
    "#### when we want to calculate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28cfc82f-421d-469b-bf33-5381bf7a0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.8259, 3.6963, 3.5944])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # will calculate dz/dx\n",
    "# now x will have gradient\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf71d7-991f-4698-a9c6-bef0da9e8cb2",
   "metadata": {},
   "source": [
    "#### imagine that we finished our computation and we need to use the results in other things so we don't need pytorch to keep calculating the gradient so we need to stop calculating the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf1aebdc-5ab4-45af-9a97-77c1d4a128db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1647, -0.6320, -1.4574])\n",
      "tensor([ 0.9442, -2.8238,  1.1483], requires_grad=True)\n",
      "tensor([ 0.9442, -2.8238,  1.1483])\n"
     ]
    }
   ],
   "source": [
    "# we can do this by using\n",
    "\n",
    "# first method is:\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "# second method is:\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3408fe-da5e-4fe7-9968-28903a396479",
   "metadata": {},
   "source": [
    "#### whenever we called the backward function then the gradient for this tensor will be accumlated into the .grad attribute which something that we don't want to happen let's see the problem and how to solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05b5f2e6-db7c-43ec-be86-4f5d8fe76a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306ca6a7-4ba5-4886-a963-f87ab945505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3df8de29-a011-46e1-8029-74ff94052cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324439e6-c7fe-48b4-a45d-c54248c14684",
   "metadata": {},
   "source": [
    "the problem is the tensor is accumulating the values is summing up so our weights or gradiens is incorrect\n",
    "to solve this we must empty the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c2f2526-4ac0-49db-93fc-a264b66c94f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    # to solve the problem\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca3c48-33f1-45ea-89d1-e82149f6b034",
   "metadata": {},
   "source": [
    "# pytorch optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01a031-83f9-4519-8948-e191622c3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(weights, lr=0.1)\n",
    "optimizer.step()\n",
    "# before we do the next iteration we will do which will clear the gradient at each iteration\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
